{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ajarun\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tf1\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\ajarun\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tf1\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\ajarun\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tf1\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\ajarun\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tf1\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\ajarun\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tf1\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\ajarun\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tf1\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using Theano backend.\n",
      "WARNING (theano.configdefaults): g++ not available, if using conda: `conda install m2w64-toolchain`\n",
      "C:\\Users\\ajarun\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tf1\\lib\\site-packages\\theano\\configdefaults.py:560: UserWarning: DeprecationWarning: there is no c++ compiler.This is deprecated and with Theano 0.11 a c++ compiler will be mandatory\n",
      "  warnings.warn(\"DeprecationWarning: there is no c++ compiler.\"\n",
      "WARNING (theano.configdefaults): g++ not detected ! Theano will be unable to execute optimized C-implementations (for both CPU and GPU) and will default to Python implementations. Performance will be severely degraded. To remove this warning, set Theano flags cxx to an empty string.\n",
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Flatten\n",
    "from keras.utils import np_utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data= pd.read_csv(r\"C:\\Users\\ajarun\\Documents\\simplilearn\\Capstone project\\Ecommerce\\train_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "def tokenize_data(dataset):\n",
    "    tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "    for i in range(dataset.shape[0]):\n",
    "       dataset[\"reviews.text\"][i] = tokenizer.tokenize(dataset[\"reviews.text\"][i])\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# In[80]:\n",
    "\n",
    "newdata=tokenize_data(data)\n",
    "\n",
    "\n",
    "# In[81]:\n",
    "\n",
    "def remove_stop_words(dataset):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    for i in range(dataset.shape[0]):\n",
    "        dataset[\"reviews\"][i] = ([token.lower() for token in dataset[\"reviews\"][i] if token not in stop_words])\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "# In[82]:\n",
    "\n",
    "def normalize(dataset):\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    for i in range(dataset.shape[0]):\n",
    "        dataset.reviews[i] = \" \".join([lemmatizer.lemmatize(token) for token in dataset.reviews[i]]).strip()\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# In[83]:\n",
    "\n",
    "import string\n",
    "def punct(dataset):\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    for i in range(dataset.shape[0]):\n",
    "        dataset.reviews[i] = [w.translate(table) for w in dataset.reviews[i]]\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# In[84]:\n",
    "\n",
    "import re\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.porter import PorterStemmer \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "# In[85]:\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "corpus = [] \n",
    "  \n",
    "for i in range(0, len(data)): \n",
    "    text = re.sub('[^a-zA-Z]', '', str(data['reviews.text'][i])) \n",
    "    text = text.lower() \n",
    "    text = text.split() \n",
    "    ps = PorterStemmer() \n",
    "    text = ''.join(text) \n",
    "    corpus.append(text) \n",
    "  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000, 3594)\n"
     ]
    }
   ],
   "source": [
    "# creating bag of words model \n",
    "X = vectorizer.fit_transform(corpus).toarray()\n",
    "print(X.shape)\n",
    "y = data['sentiment']  #.values \n",
    "num_labels = len(set(data['sentiment']))\n",
    "X = X.astype('float32')\n",
    "Y = y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "encoded_Y = encoder.transform(Y)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_y = np_utils.to_categorical(encoded_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "train_x, test_x, train_y, test_y = model_selection.train_test_split(X,dummy_y,test_size = 0.1, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "3600/3600 [==============================] - 6878s 2s/step - loss: 0.3292 - accuracy: 0.9358\n",
      "Epoch 2/2\n",
      "3600/3600 [==============================] - 6886s 2s/step - loss: 0.2453 - accuracy: 0.9367\n",
      "400/400 [==============================] - 1s 2ms/step\n",
      "\n",
      "accuracy: 94.25%\n"
     ]
    }
   ],
   "source": [
    "input_dim = X.shape[1]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(8, input_dim = input_dim , activation = 'relu'))\n",
    "model.add(Dense(10, activation = 'relu'))\n",
    "model.add(Dense(10, activation = 'relu'))\n",
    "model.add(Dense(10, activation = 'relu'))\n",
    "model.add(Dense(3, activation = 'softmax'))\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy' , optimizer = 'adam' , metrics = ['accuracy'] )\n",
    "\n",
    "model.fit(train_x, train_y, epochs = 2, batch_size = 2)\n",
    "\n",
    "scores = model.evaluate(test_x, test_y)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.recurrent import LSTM\n",
    "import keras.utils\n",
    "from keras import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout,SpatialDropout1D\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ajarun\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tf1\\lib\\site-packages\\keras\\layers\\recurrent.py:2208: UserWarning: RNN dropout is no longer supported with the Theano backend due to technical limitations. You can either set `dropout` and `recurrent_dropout` to 0, or use the TensorFlow backend.\n",
      "  'RNN dropout is no longer supported with the Theano backend '\n"
     ]
    }
   ],
   "source": [
    "model_lstm = Sequential()\n",
    "model_lstm.add(Embedding(input_dim = input_dim, output_dim = 256))\n",
    "model_lstm.add(SpatialDropout1D(0.3))\n",
    "model_lstm.add(LSTM(256, dropout = 0.3, recurrent_dropout = 0.3))\n",
    "model_lstm.add(Dense(256, activation = 'relu'))\n",
    "model_lstm.add(Dropout(0.3))\n",
    "model_lstm.add(Dense(3, activation = 'softmax'))\n",
    "model_lstm.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer='Adam',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3240 samples, validate on 360 samples\n",
      "Epoch 1/8\n"
     ]
    }
   ],
   "source": [
    "history = model_lstm.fit(train_x, train_y,\n",
    "    validation_split = 0.1,\n",
    "    epochs = 2,\n",
    "    batch_size = 512\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.plot(epochs, loss, 'g', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'y', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
